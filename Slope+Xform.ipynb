{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gen_data.py\n",
    "import numpy as np\n",
    "\n",
    "def gen_data(batch_size,num_inputs,w=[],b=[]):\n",
    "  y_output = np.zeros((batch_size,num_inputs))\n",
    "  x_input = np.zeros((batch_size,num_inputs))\n",
    "  if len(w) == 0:\n",
    "    w = np.random.randint(10, size=(num_inputs,num_inputs))\n",
    "    b = np.random.randint(5, size=(1,num_inputs))\n",
    "#  print(w)\n",
    "#  print(b)\n",
    "  for i in range(batch_size):\n",
    "    x = np.random.randint(10, size=(1,num_inputs))\n",
    "#    print('----',x)\n",
    "    y_output[i] = np.add(np.matmul(x,w),b)\n",
    "    x_input[i] = x\n",
    "#  print(y_output)\n",
    "#  print(x_input)\n",
    "  return w,b,x_input,y_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class FF():\n",
    "  \n",
    "  def init_weights(self, shape):\n",
    "    weights = tf.random_normal(shape,stddev=1)\n",
    "    return tf.Variable(weights)\n",
    "  \n",
    "  def init_b(self, num_cols):\n",
    "    return tf.Variable(tf.zeros([num_cols]))\n",
    "  \n",
    "  def __init__(self, args, graph):\n",
    "    self.graph = graph\n",
    "    self.num_inputs = int(args['num_inputs'])\n",
    "    self.num_neurons = int(args['num_neurons'])\n",
    "    self.num_layers = int(args['num_layers'])\n",
    "    #self.num_outputs = int(args.num_outputs)\n",
    "    self.num_outputs = self.num_inputs\n",
    "    self.learning_rate = float(args['learning_rate'])\n",
    "    \n",
    "    # setup inputs and outputs\n",
    "    self.x = tf.placeholder(name='x', dtype=tf.float32, shape=[None,self.num_inputs]) # shape = [batch_size, num_inputs]\n",
    "    self.y = tf.placeholder(name='y', dtype=tf.float32, shape=[None,self.num_outputs]) # shape = [batch_size, num_outputs]\n",
    "\n",
    "    # setup weights & biases\n",
    "    self.weights = []\n",
    "    self.biases = []\n",
    "    for i in range(self.num_layers):\n",
    "      if i == 0:\n",
    "        self.weights.append(self.init_weights((self.num_inputs,self.num_neurons)))\n",
    "        self.biases.append(self.init_b(self.num_outputs))\n",
    "      else: \n",
    "        self.weights.append(self.init_weights((self.num_neurons,self.num_neurons)))\n",
    "        self.biases.append(self.init_b(self.num_neurons))\n",
    "\n",
    "    # define the graph\n",
    "    layer_outputs = []\n",
    "    for i in range(self.num_layers):\n",
    "      if i == 0:\n",
    "        layer_outputs.append(tf.matmul(self.x,self.weights[i]) + self.biases[i])\n",
    "      else:\n",
    "        layer_outputs.append(tf.matmul(layer_outputs[i-1],self.weights[i]) + self.biases[i])\n",
    "    self.y_ = layer_outputs[self.num_layers - 1]\n",
    "    self.y_ = tf.identity(self.y_,name='y_')\n",
    "    self.loss = tf.losses.mean_squared_error(self.y,self.y_)\n",
    "    self.loss = tf.identity(self.loss,name='loss')\n",
    "    self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "  def run(self, test=False):\n",
    "    if test:\n",
    "      return self.y_, self.loss\n",
    "    return self.y_, self.loss, self.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# restore_graph\n",
    "def restore_graph(sess,args):\n",
    "\n",
    "  trained_model = tf.train.import_meta_graph(args['restore_path'] + '.meta')\n",
    "  #trained_model.restore(sess, tf.train.latest_checkpoint(cwd + '/checkpoints/.'))\n",
    "  trained_model.restore(sess, args['restore_path'])\n",
    "\n",
    "  w = []\n",
    "  b = []\n",
    "  model_vars_file = args['vars_file']  # need to get back w's and b's for gen_data to reproduce same slopes and y-intercepts for lines\n",
    "  with open(model_vars_file, 'rb') as f:\n",
    "    w,b = pickle.load(f)\n",
    "  print('\\n\\nModel and variables restored.\\n\\n')\n",
    "\n",
    "  if (len(w) != int(args['num_inputs'])):\n",
    "    print('Error: num_inputs length %d does not equal stored variable length %d\\n' % (int(args['num_inputs']),len(w)))\n",
    "    quit()\n",
    "\n",
    "  return trained_model, w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train\n",
    "def train(args):\n",
    "\n",
    "  graph = tf.Graph()\n",
    "  var_path = cwd + '/' + args['checkpoint_dir'] + '/variables/'\n",
    "\n",
    "  with graph.as_default():\n",
    "\n",
    "    # Graph object and scope created\n",
    "    # ...now define all parts of the graph here\n",
    "    feed_fwd_model = FF(args, graph)\n",
    "    saver = tf.train.Saver()\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Now that the graph is defined, create a session to begin running\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "      sess.run(init)\n",
    "      # Prepare to Save model\n",
    "      i = 0\n",
    "      model = 'model%s' % i\n",
    "      ckpt_file_index = Path(cwd + '/' + args['checkpoint_dir'] + '/' + model + '.ckpt.index')\n",
    "      ckpt_file = Path(cwd + '/' + args['checkpoint_dir'] + '/' + model + '.ckpt')\n",
    "      while ckpt_file_index.is_file():\n",
    "        i += 1\n",
    "        model = 'model%s' % i\n",
    "        ckpt_file_index = Path(cwd + '/' + args['checkpoint_dir'] + '/' + model + '.ckpt.index')\n",
    "      ckpt_file = Path(cwd + '/' + args['checkpoint_dir'] + '/' + model + '.ckpt')\n",
    "\n",
    "      num_epochs = int(args['num_epochs'])\n",
    "      y_acc = np.zeros((int(args['batch_size']),int(args['num_outputs'])))\n",
    "      loss = None\n",
    "      y_ = None\n",
    "      \n",
    "      w = []\n",
    "      b = []\n",
    "      if (args['restore_path'] != None):\n",
    "        trained_model_saver, w, b = restore_graph(sess,args)\n",
    "        print('...continuing training')\n",
    "\n",
    "      # guards against accidental updates to the graph which can cause graph\n",
    "      # increase and performance decay over time (with more iterations)\n",
    "      sess.graph.finalize()\n",
    "    \n",
    "      for e in range(num_epochs):\n",
    "        w, b, train_input, train_output = gen_data(int(args['batch_size']),int(args['num_inputs']), w, b)\n",
    "        y_, loss, _ = sess.run(feed_fwd_model.run(), feed_dict={feed_fwd_model.x: train_input, feed_fwd_model.y: train_output})\n",
    "        y_acc = y_\n",
    "        threshold = 1000\n",
    "        w_b_saved = False\n",
    "        if ((e % 50) == 0):\n",
    "          print('epoch: %d - loss: %2f' % (e,loss))\n",
    "          if (e > 0 and (e % threshold == 0)):\n",
    "            print('Writing checkpoint %d' % e)\n",
    "            print(train_output, w, b)\n",
    "            print('\\n')\n",
    "            print(y_acc, sess.run(feed_fwd_model.weights)[0], sess.run(feed_fwd_model.biases)[0])\n",
    "    #        save_path = saver.save(sess, str(ckpt_file), global_step=e)\n",
    "    #        if not w_b_saved:\n",
    "    #          try:\n",
    "    #            with open(var_path + model + '.pkl', 'wb') as f:\n",
    "    #              pickle.dump([w,b],f)\n",
    "    #              w_b_saved = True\n",
    "    #          except FileNotFoundError as fnf:\n",
    "    #            os.makedirs(var_path)\n",
    "    #            with open(var_path + model + '.pkl', 'wb') as f:\n",
    "    #              pickle.dump([w,b],f)\n",
    "    #              w_b_saved = True\n",
    "#      save_path = saver.save(sess, str(ckpt_file))\n",
    "#      if not w_b_saved:\n",
    "#        try:\n",
    "#          with open(var_path + model + '.pkl', 'wb') as f:\n",
    "#            pickle.dump([w,b],f)\n",
    "#        except FileNotFoundError as fnf:\n",
    "#          os.makedirs(var_path)\n",
    "#          with open(var_path + model + '.pkl', 'wb') as f:\n",
    "#            pickle.dump([w,b],f)\n",
    "#      print('Model saved to %s' % str(save_path))\n",
    "      sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test\n",
    "def test(args):\n",
    "\n",
    "  inference_graph = tf.Graph()\n",
    "  with tf.Session(graph=inference_graph) as sess:\n",
    "\n",
    "    if not args['restore_path'] or not args['vars_file']:\n",
    "      print('\\n\\n\\tSpecify a restore_path: --restore_path=<path_to_ckpt> and --vars_file=<vars_file_pathname>\\n\\n')\n",
    "      quit()\n",
    "    \n",
    "    trained_model_saver, w, b = restore_graph(sess,args)\n",
    "\n",
    "    _y_ = inference_graph.get_tensor_by_name('y_:0')\n",
    "    _loss = inference_graph.get_tensor_by_name('loss:0')\n",
    "    _x = inference_graph.get_tensor_by_name('x:0')\n",
    "    _y = inference_graph.get_tensor_by_name('y:0')\n",
    "\n",
    "    while(1):\n",
    "      w, b, train_input, train_output = gen_data(int(args['batch_size']),int(args['num_inputs']), w, b)\n",
    "      y_ = sess.run(_y_, feed_dict={_x: train_input, _y: train_output})\n",
    "      loss = sess.run(_loss, feed_dict={_x: train_input, _y: train_output})\n",
    "      y_acc = y_\n",
    "      print('Mean Squared Error Loss: %2f\\n' % loss)\n",
    "      print(train_output)\n",
    "      print('\\n')\n",
    "      print(y_acc)\n",
    "      print('\\n')\n",
    "      input('Press Enter to continue...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "epoch: 0 - loss: 634.003235\n",
      "epoch: 50 - loss: 0.226862\n",
      "epoch: 100 - loss: 0.089429\n",
      "epoch: 150 - loss: 0.241238\n",
      "epoch: 200 - loss: 0.138841\n",
      "epoch: 250 - loss: 0.113527\n",
      "epoch: 300 - loss: 0.026763\n",
      "epoch: 350 - loss: 0.100380\n",
      "epoch: 400 - loss: 0.026570\n",
      "epoch: 450 - loss: 0.047696\n",
      "epoch: 500 - loss: 0.012783\n",
      "epoch: 550 - loss: 0.071489\n",
      "epoch: 600 - loss: 0.019527\n",
      "epoch: 650 - loss: 0.001446\n",
      "epoch: 700 - loss: 0.016223\n",
      "epoch: 750 - loss: 0.012905\n",
      "epoch: 800 - loss: 0.007825\n",
      "epoch: 850 - loss: 0.007635\n",
      "epoch: 900 - loss: 0.008738\n",
      "epoch: 950 - loss: 0.004934\n",
      "epoch: 1000 - loss: 0.006285\n",
      "Writing checkpoint 1000\n",
      "(array([[22., 25.],\n",
      "       [32., 15.],\n",
      "       [22., 15.]]), array([[2, 4],\n",
      "       [6, 2]]), array([[0, 1]]))\n",
      "\n",
      "\n",
      "(array([[22.076105, 24.940096],\n",
      "       [32.083195, 14.934516],\n",
      "       [22.10282 , 14.919067]], dtype=float32), array([[1.9831576, 4.013257 ],\n",
      "       [5.9799595, 2.0157743]], dtype=float32), array([0.18252416, 0.8563306 ], dtype=float32))\n",
      "epoch: 1050 - loss: 0.001483\n",
      "epoch: 1100 - loss: 0.001703\n",
      "epoch: 1150 - loss: 0.002897\n"
     ]
    }
   ],
   "source": [
    "#import argparse\n",
    "#import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "NUM_INPUTS = 2\n",
    "NUM_OUTPUTS = NUM_INPUTS\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "#def main():\n",
    "#  parser = argparse.ArgumentParser()\n",
    "#  parser.add_argument('--mode', default='train')\n",
    "#  parser.add_argument('--num_inputs', default=NUM_INPUTS)\n",
    "#  parser.add_argument('--batch_size', default=3)\n",
    "#  parser.add_argument('--num_neurons', default=2)\n",
    "#  parser.add_argument('--num_layers', default=1)\n",
    "#  parser.add_argument('--num_outputs', default=NUM_OUTPUTS)\n",
    "#  parser.add_argument('--learning_rate', default=0.001)\n",
    "#  parser.add_argument('--num_epochs', default=10)\n",
    "#  parser.add_argument('--checkpoint_dir', default='./checkpoints')\n",
    "#  parser.add_argument('--restore_path', default=None)\n",
    "#  parser.add_argument('--vars_file', default=None)\n",
    "\n",
    "#  args = parser.parse_args()\n",
    "mode = 'train'\n",
    "num_inputs = 2\n",
    "batch_size = 3\n",
    "num_neurons = 2\n",
    "num_layers = 1\n",
    "num_outputs = num_inputs\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1200\n",
    "checkpoint_dir = './checkpoints'\n",
    "restore_path = None\n",
    "vars_file = None\n",
    "\n",
    "args = {'mode': mode,\\\n",
    "        'num_inputs': num_inputs,\\\n",
    "        'batch_size': batch_size,\\\n",
    "        'num_neurons': num_neurons,\\\n",
    "        'num_layers': num_layers,\\\n",
    "        'num_outputs': num_outputs,\\\n",
    "        'learning_rate': learning_rate,\\\n",
    "        'num_epochs': num_epochs,\\\n",
    "        'checkpoint_dir': checkpoint_dir,\\\n",
    "        'restore_path': restore_path,\\\n",
    "        'vars_file': vars_file}\n",
    "\n",
    "if args['mode'] == 'train':\n",
    "  print('Training...')\n",
    "  train(args)\n",
    "elif args['mode'] == 'test':\n",
    "  test(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
